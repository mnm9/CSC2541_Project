{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tqdm\n",
    "import ast\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utilities import helper_functions, splitting, augmentations, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrandsDataset(Dataset):\n",
    "    '''\n",
    "    Brands Dataset\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, img_files, views, labels, rgb, view_append):\n",
    "        '''\n",
    "        Initialize the dataset with image files, their corresponding Y labels (in encoded numpy format), the \n",
    "        function to pre-process the image, the function to apply transformations (data augmentation) to the image, \n",
    "        whether or not you need to convert your images to be converted to rgb (our images are grayscale and we \n",
    "        needed to duplicate our grayscale images along 3 channels to convert them to RGB so that they can be input \n",
    "        into a pretrained model), and whether you need to flatten (for baselines) the input before feeding it into \n",
    "        the model\n",
    "        '''\n",
    "        assert len(img_files) == len(labels), \"Number of files should match number of targets\"\n",
    "        \n",
    "        self.img_files = img_files\n",
    "        self.labels = labels\n",
    "        self.rgb = rgb\n",
    "        self.views = views\n",
    "        self.view_append = view_append\n",
    "    \n",
    "    def load_dicom(self, img_path):\n",
    "        '''\n",
    "        This function loads an image from a DICOM path. If there is an error with the path, it will print error and \n",
    "        return a 256x256 array of zeros\n",
    "        '''\n",
    "        try:\n",
    "            image_info = pydicom.dcmread(\"../\"+ img_path)\n",
    "            actual_image = image_info.pixel_array\n",
    "            \n",
    "        except:\n",
    "            print(f\"Something went wrong with reading file {img_path}\")\n",
    "            actual_image = np.zeros((256,256))\n",
    "        \n",
    "        actual_image = helper_functions.prepare_image(actual_image, rgb=self.rgb, channels_first=True).flatten()\n",
    "        \n",
    "        return actual_image\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Get a unique item from the dataset according to index. This is required when building a custom dataloader\n",
    "        '''\n",
    "        X = self.load_dicom(self.img_files[index])\n",
    "        X = X/255.\n",
    "        if self.view_append:\n",
    "            if self.views[index] == \"AP\":\n",
    "                view = 1\n",
    "            elif self.views[index] == \"L\":\n",
    "                view = 2\n",
    "            X = np.append(X, view)\n",
    "            \n",
    "        return X\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Length of the dataset. This is required when building a custom dataloader\n",
    "        '''\n",
    "        return len(self.img_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGridSearch(data, suffix, view):\n",
    "    print(\"Preparing Datasets and Models...\")\n",
    "    train_set = data.copy()\n",
    "    # Creating dataset\n",
    "    train_dataset = BrandsDataset(train_set['filepath'], train_set['View'], train_set['Label'], rgb = False, view_append=view)\n",
    "\n",
    "    # Loading img_paths in dataset to create a Pandas Dataframe\n",
    "    datasets = {'train': train_dataset}\n",
    "    tensor_data = {'train': []}\n",
    "    \n",
    "    for item in datasets.keys():\n",
    "        for row in range(len(datasets[item])):\n",
    "            img_ = datasets[item].__getitem__(row)\n",
    "            if not np.any(img_):\n",
    "                print(\"Moving On...\")\n",
    "                continue\n",
    "            else:\n",
    "                tensor_data[item].append(img_)\n",
    "\n",
    "    # Converting final dataset to NumPy array\n",
    "    X_train_final = np.array(tensor_data['train'])\n",
    "\n",
    "    # Creating a Logistic Regression Model\n",
    "    lr = LogisticRegression(random_state=0, max_iter=50, class_weight = 'balanced', solver = 'liblinear', multi_class = 'ovr')\n",
    "\n",
    "    # Selecting hyperparameters to fine tune\n",
    "    parameter_grid = {\n",
    "        'C':[0.01, 0.1, 1],\n",
    "        'penalty': ['l1', 'l2']\n",
    "    }\n",
    "    \n",
    "    # 5-fold Cross Val Grid Search using weighted F1\n",
    "    clf = GridSearchCV(lr, parameter_grid, scoring = \"f1_weighted\", cv = 5, n_jobs=-1)\n",
    "    \n",
    "    print(\"Doing Grid Search and Selecting Best Params...\")\n",
    "    # Fitting the final model\n",
    "    since = time.time()\n",
    "    clf.fit(X_train_final, train_set['Label'])\n",
    "    elapsed = time.time() - since\n",
    "    print(\"Elapsed:\", elapsed)\n",
    "    \n",
    "    df = pd.DataFrame(clf.cv_results_).sort_values(by=['param_penalty','mean_test_score'], ascending=[True, False]).reset_index(drop=True)\n",
    "    df.to_csv(f\"results/baselines/tuning/logregtuning-{suffix}-view{view}.csv\", index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Master_Posterior_HDW.csv\")\n",
    "suffix = 'posterior'\n",
    "view=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Datasets and Models...\n",
      "Doing Grid Search and Selecting Best Params...\n",
      "Elapsed: 24.347995042800903\n"
     ]
    }
   ],
   "source": [
    "runGridSearch(data, suffix, view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Master_Posterior_HDW.csv\")\n",
    "suffix = 'posterior'\n",
    "view=False\n",
    "regularization_type = 'l2'\n",
    "iterations = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWithSignificance(data, suffix, view, regularization_type, iterations):\n",
    "    # Selecting best hyperparameters\n",
    "    df = pd.read_csv(f\"results/baselines/tuning/logregtuning-{suffix}-view{view}.csv\")\n",
    "    if regularization_type == 'l1':\n",
    "        best_c = df.loc[0, 'param_C'] # Best L1 is at the top according to system above. This is hard coding!\n",
    "    elif regularization_type == 'l2':\n",
    "        best_c = df.loc[3, 'param_C'] # best L2 is in row 3 according to system above. This is hard coding!\n",
    "    print(\"Best Regularization\", best_c)\n",
    "    \n",
    "    # Intializing storage variables\n",
    "    f1_scores = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    aucs = []\n",
    "    confusion_matrices = []\n",
    "    \n",
    "    # Getting statistical significance\n",
    "    for i in tqdm.notebook.tqdm(range(iterations)):\n",
    "        print(\"Setting Up Data...\")\n",
    "        # Splitting dataset, combining train and val into one train set\n",
    "        train_set, val_set, test_set = splitting.split_data_2(data, suffix, return_data=True, save_data = False)\n",
    "        train_set = pd.concat([train_set, val_set], axis=0).reset_index(drop=True)\n",
    "        \n",
    "        # Creating a PyTorch dataset\n",
    "        train_dataset = BrandsDataset(train_set['filepath'], train_set['View'], train_set['Label'], rgb = False, view_append=view)\n",
    "        test_dataset = BrandsDataset(test_set['filepath'], test_set['View'], test_set['Label'], rgb = False, view_append=view)\n",
    "\n",
    "        # Loading img_paths in dataset to create a Pandas Dataframe\n",
    "        datasets = {'train': train_dataset, 'test': test_dataset}\n",
    "        tensor_data = {'train': [], 'test': []}\n",
    "        \n",
    "        # Going through Pytorch datasets and appending to a list\n",
    "        for item in datasets.keys():\n",
    "            for row in range(len(datasets[item])):\n",
    "                img_ = datasets[item].__getitem__(row)\n",
    "                if not np.any(img_):\n",
    "                    print(\"Moving On...\")\n",
    "                    continue\n",
    "                else:\n",
    "                    tensor_data[item].append(img_)\n",
    "\n",
    "        # Converting final list datasets to NumPy arrays\n",
    "        X_train_final = np.array(tensor_data['train'])\n",
    "        print(\"Train Shape\", X_train_final.shape)\n",
    "        print(\"Unique Train Brands:\", train_set['Label'].unique())\n",
    "        X_test_final = np.array(tensor_data['test']) \n",
    "        print(\"Test Shape\", X_test_final.shape)\n",
    "        print(\"Unique Test Brands:\", test_set['Label'].unique())\n",
    "        \n",
    "        # Training the model\n",
    "        print(f\"Training Final {regularization_type} Model...\")\n",
    "        lr = LogisticRegression(penalty = regularization_type, C = best_c, random_state=0, max_iter=10000, \n",
    "                                   class_weight = 'balanced', solver = 'liblinear', multi_class = 'ovr') \n",
    "        lr.fit(X_train_final, train_set['Label'])\n",
    "        \n",
    "        # Evaluating the model\n",
    "        predictions_, probabilities_ = lr.predict(X_test_final), lr.predict_proba(X_test_final)\n",
    "        \n",
    "        # Calculating metrics\n",
    "        f1_score, precision, recall, auc, _ = metrics.metrics_function(y_predicted=predictions_, \n",
    "                                                                        y_probs=probabilities_, \n",
    "                                                                        y_true=test_set['Label'])\n",
    "        \n",
    "        # Appending metrics\n",
    "        f1_scores.append(f1_score)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        aucs.append(auc)\n",
    "        confusion_matrices.append(_)\n",
    "    \n",
    "    # Compiling results from all trials and exporting to a CSV\n",
    "    print(f\"Compiling Results from {iterations} trials...\")\n",
    "    compiled_numeric = metrics.compile_numeric_results(f1_scores, precisions, recalls, aucs)\n",
    "    compiled_numeric.index.name = 'Score'\n",
    "    compiled_numeric.to_csv(f\"results/baselines/metrics/{regularization_type}logreg-view{view}-{suffix}\"+ \"_numeric.csv\", index=True)\n",
    "    \n",
    "    return confusion_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Regularization 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee0978f18594025921512af7a7e83f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Up Data...\n",
      "Train Shape (159, 65536)\n",
      "Unique Train Brands: [2. 0. 1. 3.]\n",
      "Test Shape (71, 65536)\n",
      "Unique Test Brands: [0. 1. 2. 3.]\n",
      "Training Final l2 Model...\n",
      "Setting Up Data...\n",
      "Train Shape (163, 65536)\n",
      "Unique Train Brands: [2. 0. 3. 1.]\n",
      "Test Shape (67, 65536)\n",
      "Unique Test Brands: [0. 1. 2. 3.]\n",
      "Training Final l2 Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-utoronto_spine/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Up Data...\n",
      "Train Shape (178, 65536)\n",
      "Unique Train Brands: [2. 0. 3. 1.]\n",
      "Test Shape (52, 65536)\n",
      "Unique Test Brands: [2. 3. 1. 0.]\n",
      "Training Final l2 Model...\n",
      "Setting Up Data...\n",
      "Train Shape (165, 65536)\n",
      "Unique Train Brands: [2. 0. 3. 1.]\n",
      "Test Shape (65, 65536)\n",
      "Unique Test Brands: [3. 0. 1. 2.]\n",
      "Training Final l2 Model...\n",
      "Setting Up Data...\n",
      "Train Shape (160, 65536)\n",
      "Unique Train Brands: [2. 0. 1. 3.]\n",
      "Test Shape (70, 65536)\n",
      "Unique Test Brands: [1. 0. 3. 2.]\n",
      "Training Final l2 Model...\n",
      "Setting Up Data...\n",
      "Train Shape (165, 65536)\n",
      "Unique Train Brands: [2. 0. 1. 3.]\n",
      "Test Shape (65, 65536)\n",
      "Unique Test Brands: [3. 1. 2. 0.]\n",
      "Training Final l2 Model...\n",
      "Setting Up Data...\n",
      "Train Shape (182, 65536)\n",
      "Unique Train Brands: [2. 0. 1. 3.]\n",
      "Test Shape (48, 65536)\n",
      "Unique Test Brands: [2. 3. 0. 1.]\n",
      "Training Final l2 Model...\n",
      "Setting Up Data...\n",
      "Train Shape (178, 65536)\n",
      "Unique Train Brands: [2. 0. 1. 3.]\n",
      "Test Shape (52, 65536)\n",
      "Unique Test Brands: [0. 2. 3. 1.]\n",
      "Training Final l2 Model...\n",
      "Setting Up Data...\n",
      "Train Shape (167, 65536)\n",
      "Unique Train Brands: [2. 0. 1. 3.]\n",
      "Test Shape (63, 65536)\n",
      "Unique Test Brands: [0. 3. 1. 2.]\n",
      "Training Final l2 Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-utoronto_spine/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Up Data...\n",
      "Train Shape (174, 65536)\n",
      "Unique Train Brands: [0. 1. 3. 2.]\n",
      "Test Shape (56, 65536)\n",
      "Unique Test Brands: [0. 2. 1. 3.]\n",
      "Training Final l2 Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-utoronto_spine/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Up Data...\n",
      "Train Shape (163, 65536)\n",
      "Unique Train Brands: [0. 3. 1. 2.]\n",
      "Test Shape (67, 65536)\n",
      "Unique Test Brands: [0. 1. 2. 3.]\n",
      "Training Final l2 Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-utoronto_spine/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Up Data...\n",
      "Train Shape (178, 65536)\n",
      "Unique Train Brands: [0. 1. 2. 3.]\n",
      "Test Shape (52, 65536)\n",
      "Unique Test Brands: [0. 3. 2. 1.]\n",
      "Training Final l2 Model...\n",
      "Setting Up Data...\n",
      "Train Shape (163, 65536)\n",
      "Unique Train Brands: [0. 3. 1. 2.]\n",
      "Test Shape (67, 65536)\n",
      "Unique Test Brands: [2. 0. 3. 1.]\n",
      "Training Final l2 Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-utoronto_spine/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Up Data...\n",
      "Train Shape (168, 65536)\n",
      "Unique Train Brands: [0. 2. 3. 1.]\n",
      "Test Shape (62, 65536)\n",
      "Unique Test Brands: [0. 3. 2. 1.]\n",
      "Training Final l2 Model...\n",
      "Setting Up Data...\n",
      "Train Shape (155, 65536)\n",
      "Unique Train Brands: [0. 1. 2. 3.]\n",
      "Test Shape (75, 65536)\n",
      "Unique Test Brands: [2. 3. 1. 0.]\n",
      "Training Final l2 Model...\n",
      "Setting Up Data...\n",
      "Train Shape (156, 65536)\n",
      "Unique Train Brands: [0. 2. 3. 1.]\n",
      "Test Shape (74, 65536)\n",
      "Unique Test Brands: [2. 0. 1. 3.]\n",
      "Training Final l2 Model...\n",
      "Setting Up Data...\n",
      "Train Shape (181, 65536)\n",
      "Unique Train Brands: [2. 0. 3. 1.]\n",
      "Test Shape (49, 65536)\n",
      "Unique Test Brands: [0. 1. 2. 3.]\n",
      "Training Final l2 Model...\n",
      "Setting Up Data...\n",
      "Train Shape (172, 65536)\n",
      "Unique Train Brands: [2. 0. 1. 3.]\n",
      "Test Shape (58, 65536)\n",
      "Unique Test Brands: [0. 2. 3. 1.]\n",
      "Training Final l2 Model...\n",
      "Setting Up Data...\n",
      "Train Shape (168, 65536)\n",
      "Unique Train Brands: [0. 2. 3. 1.]\n",
      "Test Shape (62, 65536)\n",
      "Unique Test Brands: [2. 3. 0. 1.]\n",
      "Training Final l2 Model...\n",
      "Setting Up Data...\n",
      "Train Shape (154, 65536)\n",
      "Unique Train Brands: [0. 2. 3. 1.]\n",
      "Test Shape (76, 65536)\n",
      "Unique Test Brands: [2. 0. 1. 3.]\n",
      "Training Final l2 Model...\n",
      "\n",
      "Compiling Results from 20 trials...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[27, 12,  5,  3],\n",
       "        [ 6,  1,  0,  0],\n",
       "        [ 5,  1,  0,  1],\n",
       "        [ 8,  2,  0,  0]]), array([[32, 10,  4,  0],\n",
       "        [ 9,  1,  0,  0],\n",
       "        [ 1,  0,  0,  0],\n",
       "        [ 9,  1,  0,  0]]), array([[19,  7, 11,  1],\n",
       "        [ 6,  0,  0,  1],\n",
       "        [ 0,  0,  1,  0],\n",
       "        [ 2,  0,  4,  0]]), array([[24,  3,  3,  4],\n",
       "        [16,  0,  0,  2],\n",
       "        [ 6,  0,  0,  1],\n",
       "        [ 2,  0,  4,  0]]), array([[26, 12,  3,  6],\n",
       "        [ 9,  1,  0,  0],\n",
       "        [ 7,  0,  0,  0],\n",
       "        [ 6,  0,  0,  0]]), array([[26,  8,  0,  0],\n",
       "        [18,  0,  0,  0],\n",
       "        [ 6,  0,  0,  1],\n",
       "        [ 3,  0,  3,  0]]), array([[22,  6,  2,  4],\n",
       "        [ 6,  0,  0,  1],\n",
       "        [ 1,  0,  0,  0],\n",
       "        [ 4,  0,  2,  0]]), array([[25, 11,  5,  2],\n",
       "        [ 1,  0,  1,  0],\n",
       "        [ 1,  0,  0,  0],\n",
       "        [ 5,  1,  0,  0]]), array([[30,  2,  0,  0],\n",
       "        [18,  0,  0,  0],\n",
       "        [ 7,  0,  0,  0],\n",
       "        [ 5,  1,  0,  0]]), array([[20,  5,  0,  2],\n",
       "        [18,  0,  0,  0],\n",
       "        [ 1,  0,  0,  0],\n",
       "        [10,  0,  0,  0]]), array([[32, 10,  4,  0],\n",
       "        [ 9,  1,  0,  0],\n",
       "        [ 1,  0,  0,  0],\n",
       "        [ 9,  1,  0,  0]]), array([[18,  9,  7,  3],\n",
       "        [ 1,  0,  0,  1],\n",
       "        [ 2,  0,  1,  4],\n",
       "        [ 2,  0,  4,  0]]), array([[24,  9,  0,  6],\n",
       "        [ 5,  1,  0,  1],\n",
       "        [13,  1,  0,  1],\n",
       "        [ 4,  1,  0,  1]]), array([[25, 15,  3,  4],\n",
       "        [ 2,  0,  0,  0],\n",
       "        [ 7,  0,  0,  0],\n",
       "        [ 5,  1,  0,  0]]), array([[27,  1,  4,  4],\n",
       "        [18,  0,  0,  0],\n",
       "        [13,  0,  0,  2],\n",
       "        [ 5,  1,  0,  0]]), array([[25,  2,  3,  1],\n",
       "        [18,  0,  0,  0],\n",
       "        [12,  0,  0,  3],\n",
       "        [10,  0,  0,  0]]), array([[26,  4,  1,  1],\n",
       "        [ 9,  1,  0,  0],\n",
       "        [ 1,  0,  0,  0],\n",
       "        [ 5,  1,  0,  0]]), array([[28, 10,  6,  5],\n",
       "        [ 1,  0,  1,  0],\n",
       "        [ 1,  0,  0,  0],\n",
       "        [ 2,  0,  4,  0]]), array([[15, 17,  3,  4],\n",
       "        [ 2,  0,  0,  0],\n",
       "        [10,  2,  0,  3],\n",
       "        [ 4,  2,  0,  0]]), array([[30,  8,  4,  2],\n",
       "        [ 5,  2,  0,  0],\n",
       "        [ 9,  2,  0,  4],\n",
       "        [ 8,  2,  0,  0]])]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainWithSignificance(data, suffix, view, regularization_type, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = 'posterior'\n",
    "view=True\n",
    "regularization_type = 'l2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Confidence Intervals</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.398056</td>\n",
       "      <td>[0.3459387916012953, 0.45017255237060044]</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.391110</td>\n",
       "      <td>[0.32463881671766814, 0.4575813366430911]</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.426840</td>\n",
       "      <td>[0.38596863855614316, 0.4677116101473494]</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUC</td>\n",
       "      <td>0.402446</td>\n",
       "      <td>[0.357630703735772, 0.44726066622320093]</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score      Mean                       Confidence Intervals  Support\n",
       "0         F1  0.398056  [0.3459387916012953, 0.45017255237060044]       20\n",
       "1  Precision  0.391110  [0.32463881671766814, 0.4575813366430911]       20\n",
       "2     Recall  0.426840  [0.38596863855614316, 0.4677116101473494]       20\n",
       "3        AUC  0.402446   [0.357630703735772, 0.44726066622320093]       20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.read_csv(f\"results/baselines/metrics/{regularization_type}logreg-view{view}-{suffix}\"+ \"_numeric.csv\")\n",
    "results['Confidence Intervals'] =results['Confidence Intervals'].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Mean</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.398056</td>\n",
       "      <td>[-0.052116880384652564, 0.052116880384652564]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.391110</td>\n",
       "      <td>[-0.06647125996271147, 0.06647125996271147]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.426840</td>\n",
       "      <td>[-0.040871485795603124, 0.040871485795603124]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUC</td>\n",
       "      <td>0.402446</td>\n",
       "      <td>[-0.044814981243714525, 0.044814981243714413]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score      Mean                                              0\n",
       "0         F1  0.398056  [-0.052116880384652564, 0.052116880384652564]\n",
       "1  Precision  0.391110    [-0.06647125996271147, 0.06647125996271147]\n",
       "2     Recall  0.426840  [-0.040871485795603124, 0.040871485795603124]\n",
       "3        AUC  0.402446  [-0.044814981243714525, 0.044814981243714413]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([results[['Score','Mean']], results['Confidence Intervals'] - results['Mean']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = 'anterior'\n",
    "view=True\n",
    "regularization_type = 'l2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Regularization 0.1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f\"results/baselines/tuning/logregtuning-{suffix}-view{view}.csv\")\n",
    "if regularization_type == 'l1':\n",
    "    best_c = df.loc[0, 'param_C'] # Best L1 is at the top according to system above. This is hard coding!\n",
    "elif regularization_type == 'l2':\n",
    "    best_c = df.loc[3, 'param_C'] # best L2 is in row 3 according to system above. This is hard coding!\n",
    "print(\"Best Regularization\", best_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
