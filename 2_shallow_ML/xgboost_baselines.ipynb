{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import sklearn.utils\n",
    "import xgboost as xgb\n",
    "import tqdm\n",
    "import ast\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utilities import helper_functions, splitting, augmentations, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrandsDataset(Dataset):\n",
    "    '''\n",
    "    Brands Dataset\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, img_files, views, labels, rgb, view_append):\n",
    "        '''\n",
    "        Initialize the dataset with image files, their corresponding Y labels (in encoded numpy format), the \n",
    "        function to pre-process the image, the function to apply transformations (data augmentation) to the image, \n",
    "        whether or not you need to convert your images to be converted to rgb (our images are grayscale and we \n",
    "        needed to duplicate our grayscale images along 3 channels to convert them to RGB so that they can be input \n",
    "        into a pretrained model), and whether you need to flatten (for baselines) the input before feeding it into \n",
    "        the model\n",
    "        '''\n",
    "        assert len(img_files) == len(labels), \"Number of files should match number of targets\"\n",
    "        \n",
    "        self.img_files = img_files\n",
    "        self.labels = labels\n",
    "        self.views = views\n",
    "        self.rgb = rgb\n",
    "        self.view_append = view_append\n",
    "    \n",
    "    def load_dicom(self, img_path):\n",
    "        '''\n",
    "        This function loads an image from a DICOM path. If there is an error with the path, it will print error and \n",
    "        return a 256x256 array of zeros\n",
    "        '''\n",
    "        try:\n",
    "            image_info = pydicom.dcmread(\"../\"+ img_path)\n",
    "            actual_image = image_info.pixel_array\n",
    "            \n",
    "        except:\n",
    "            print(f\"Something went wrong with reading file {img_path}\")\n",
    "            actual_image = np.zeros((256,256))\n",
    "        \n",
    "        actual_image = helper_functions.prepare_image(actual_image, rgb=self.rgb, channels_first=True).flatten()\n",
    "\n",
    "        return actual_image\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Get a unique item from the dataset according to index. This is required when building a custom dataloader\n",
    "        '''\n",
    "        X = self.load_dicom(self.img_files[index])\n",
    "        X = X/255.\n",
    "        if self.view_append:\n",
    "            if self.views[index] == \"AP\":\n",
    "                view = 1\n",
    "            elif self.views[index] == \"L\":\n",
    "                view = 2\n",
    "            X = np.append(X, view)\n",
    "            \n",
    "        return X\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Length of the dataset. This is required when building a custom dataloader\n",
    "        '''\n",
    "        return len(self.img_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGridSearch(data, suffix, view):\n",
    "    print(\"Preparing Datasets and Models...\")\n",
    "    train_set = data.copy()\n",
    "    # Creating dataset\n",
    "    train_dataset = BrandsDataset(train_set['filepath'], train_set['View'], train_set['Label'], rgb = False, view_append=view)\n",
    "\n",
    "    # Creating dataset\n",
    "    train_dataset = BrandsDataset(train_set['filepath'], train_set['View'], train_set['Label'], rgb = False, view_append=view)\n",
    "\n",
    "    # Loading img_paths in dataset to create a Pandas Dataframe\n",
    "    datasets = {'train': train_dataset}\n",
    "    tensor_data = {'train': []}\n",
    "    \n",
    "    for item in datasets.keys():\n",
    "        for row in range(len(datasets[item])):\n",
    "            img_ = datasets[item].__getitem__(row)\n",
    "            if not np.any(img_):\n",
    "                print(\"Moving On...\")\n",
    "                continue\n",
    "            else:\n",
    "                tensor_data[item].append(img_)\n",
    "\n",
    "    # Converting final dataset to NumPy array\n",
    "    X_train_final = np.array(tensor_data['train'])\n",
    "    \n",
    "    # Weighting imablanced classes\n",
    "    weightings = sklearn.utils.class_weight.compute_class_weight('balanced', np.sort(train_set['Label'].unique()), train_set['Label'])\n",
    "    weights = pd.DataFrame(weightings, index=np.sort(train_set['Label'].unique())).reset_index()\n",
    "    weights = pd.merge(left=train_set['Label'], right=weights, how='left', left_on='Label', right_on = 'index')[0]    \n",
    "\n",
    "    # XGBoost train and test sets\n",
    "    dtrain = xgb.DMatrix(X_train_final, label=train_set['Label'], weight = weights)\n",
    "    \n",
    "    # A parameter grid for XGBoost\n",
    "    params = {\n",
    "        'min_child_weight': 5, \n",
    "        'gamma': 0.5, \n",
    "        'subsample': 1, \n",
    "        'max_depth': 4, \n",
    "        'objective': 'multi:softmax',\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'num_class': len(train_set['Label'].unique())\n",
    "        }\n",
    "    num_boost_round = 999\n",
    "    gridsearch_params = [\n",
    "    (max_depth, gamma, subsample)\n",
    "    for max_depth in [4,6, 8]\n",
    "    for gamma in [0.5,2]\n",
    "    for subsample in [0.8, 1]\n",
    "    ]\n",
    "    \n",
    "    min_mlogloss = float(\"Inf\")\n",
    "    best_params = None\n",
    "    for max_depth, gamma,subsample in gridsearch_params:\n",
    "        print(\"CV with max_depth={}, min_gamma={}, min_subsample={}\".format(\n",
    "                                 max_depth,\n",
    "                                 gamma, subsample))\n",
    "        # Update our parameters\n",
    "        params['max_depth'] = max_depth\n",
    "        params['gamma'] = gamma\n",
    "        params['subsample'] = subsample\n",
    "        # Run CV\n",
    "        cv_results = xgb.cv(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=num_boost_round,\n",
    "            seed=42,\n",
    "            nfold=5,\n",
    "            metrics={'mlogloss'},\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "        # Update best mlogloss\n",
    "        mean_mlogloss = cv_results['test-mlogloss-mean'].min()\n",
    "        boost_rounds = cv_results['test-mlogloss-mean'].idxmin()\n",
    "        print(\"\\tMLOGLOSS {} for {} rounds\".format(mean_mlogloss, boost_rounds))\n",
    "        if mean_mlogloss < min_mlogloss:\n",
    "            min_mlogloss = mean_mlogloss\n",
    "            best_params = (max_depth,gamma, subsample)\n",
    "    print(\"Best params: {}, {}, {} mlogloss: {}\".format(best_params[0], best_params[1], best_params[2], min_mlogloss))\n",
    "    \n",
    "    params['max_depth'] = best_params[0]\n",
    "    params['gamma'] = best_params[1]\n",
    "    params['subsample'] = best_params[2]\n",
    "    \n",
    "    pd.DataFrame(params, index=[0]).to_csv(f\"results/baselines/tuning/xgboosttuning-{suffix}-view{view}.csv\")\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Master_Posterior_HDW.csv\")\n",
    "suffix = 'posterior'\n",
    "view=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Datasets and Models...\n",
      "CV with max_depth=4, min_gamma=0.5, min_subsample=0.8\n",
      "\tMLOGLOSS 1.3995362 for 0 rounds\n",
      "CV with max_depth=4, min_gamma=0.5, min_subsample=1\n",
      "\tMLOGLOSS 1.3685988000000002 for 1 rounds\n",
      "CV with max_depth=4, min_gamma=2, min_subsample=0.8\n",
      "\tMLOGLOSS 1.4019492 for 0 rounds\n",
      "CV with max_depth=4, min_gamma=2, min_subsample=1\n",
      "\tMLOGLOSS 1.3703438000000001 for 1 rounds\n",
      "CV with max_depth=6, min_gamma=0.5, min_subsample=0.8\n",
      "\tMLOGLOSS 1.4002912 for 0 rounds\n",
      "CV with max_depth=6, min_gamma=0.5, min_subsample=1\n",
      "\tMLOGLOSS 1.3689008 for 0 rounds\n",
      "CV with max_depth=6, min_gamma=2, min_subsample=0.8\n",
      "\tMLOGLOSS 1.4019492 for 0 rounds\n",
      "CV with max_depth=6, min_gamma=2, min_subsample=1\n",
      "\tMLOGLOSS 1.367848 for 1 rounds\n",
      "CV with max_depth=8, min_gamma=0.5, min_subsample=0.8\n",
      "\tMLOGLOSS 1.4002914 for 0 rounds\n",
      "CV with max_depth=8, min_gamma=0.5, min_subsample=1\n",
      "\tMLOGLOSS 1.3689008 for 0 rounds\n",
      "CV with max_depth=8, min_gamma=2, min_subsample=0.8\n",
      "\tMLOGLOSS 1.4019494 for 0 rounds\n",
      "CV with max_depth=8, min_gamma=2, min_subsample=1\n",
      "\tMLOGLOSS 1.3678478 for 1 rounds\n",
      "Best params: 8, 2, 1 mlogloss: 1.3678478\n"
     ]
    }
   ],
   "source": [
    "params = runGridSearch(data, suffix, view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainin Final Models Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Master_Anterior_HDW.csv\")\n",
    "suffix = 'anterior'\n",
    "view=True\n",
    "iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWithSignificance(data, suffix, view, iterations):\n",
    "    # Selecting best hyperparameters\n",
    "    df = pd.read_csv(f\"results/baselines/tuning/xgboosttuning-{suffix}-view{view}.csv\")\n",
    "    best_params = df[['max_depth', 'subsample', 'gamma']].to_dict('records')[0] # Hard coding!\n",
    "    params = {'min_child_weight': 5, \n",
    "                    'objective': 'multi:softprob', \n",
    "                    'eval_metric': 'mlogloss', \n",
    "                    'num_class': len(data['Label'].unique())}\n",
    "    params.update(best_params)\n",
    "    print(\"Parameters:\", params)\n",
    "    \n",
    "    # Initializing storage variables\n",
    "    f1_scores = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    aucs = []\n",
    "    \n",
    "    # Getting statistical significance\n",
    "    for i in tqdm.notebook.tqdm(range(iterations)):\n",
    "        print(\"Setting Up Data...\")\n",
    "        # Splitting dataset, combining train and val into one train set\n",
    "        train_set, val_set, test_set = splitting.split_data_2(data, suffix, return_data=True, save_data = False)\n",
    "        \n",
    "        # Creating a PyTorch dataset\n",
    "        train_dataset = BrandsDataset(train_set['filepath'], train_set['View'], train_set['Label'], rgb = False, view_append=view)\n",
    "        val_dataset = BrandsDataset(val_set['filepath'], val_set['View'], val_set['Label'], rgb = False, view_append=view)\n",
    "        test_dataset = BrandsDataset(test_set['filepath'], test_set['View'], test_set['Label'], rgb = False, view_append=view)\n",
    "\n",
    "        # Loading img_paths in dataset to create a Pandas Dataframe\n",
    "        datasets = {'train': train_dataset, 'val': val_dataset, 'test': test_dataset}\n",
    "        tensor_data = {'train': [], 'val': [], 'test': []}\n",
    "        \n",
    "        # Going through Pytorch datasets and appending to a list\n",
    "        for item in datasets.keys():\n",
    "            for row in range(len(datasets[item])):\n",
    "                img_ = datasets[item].__getitem__(row)\n",
    "                if not np.any(img_):\n",
    "                    print(\"Moving On...\")\n",
    "                    continue\n",
    "                else:\n",
    "                    tensor_data[item].append(img_)\n",
    "\n",
    "        # Converting final list datasets to NumPy arrays\n",
    "        X_train_final = np.array(tensor_data['train'])\n",
    "        print(\"Train Shape\", X_train_final.shape)\n",
    "        X_val_final = np.array(tensor_data['val'])\n",
    "        print(\"Val Shape\", X_val_final.shape)        \n",
    "        X_test_final = np.array(tensor_data['test']) \n",
    "        print(\"Test Shape\", X_test_final.shape)\n",
    "        \n",
    "        # Weighting imablanced classes\n",
    "        weightings = sklearn.utils.class_weight.compute_class_weight('balanced', np.sort(train_set['Label'].unique()), train_set['Label'])\n",
    "        weights = pd.DataFrame(weightings, index=np.sort(train_set['Label'].unique())).reset_index()\n",
    "        weights = pd.merge(left=train_set['Label'], right=weights, how='left', left_on='Label', right_on = 'index')[0] \n",
    "        \n",
    "        # XGBoost train, val and test sets\n",
    "        dtrain = xgb.DMatrix(X_train_final, label=train_set['Label'], weight = weights)\n",
    "        dval = xgb.DMatrix(X_val_final, label=val_set['Label'])\n",
    "        dtest = xgb.DMatrix(X_test_final, label=test_set['Label'])\n",
    "        \n",
    "        # Training the model\n",
    "        print(\"Training Final XGB Model...\")\n",
    "        model = xgb.train(params,\n",
    "                          dtrain,\n",
    "                          num_boost_round=999,\n",
    "                          evals=[(dval, \"Val\")],\n",
    "                          early_stopping_rounds=10)\n",
    "        print(\"Best MLogloss: {:.2f} with {} rounds\".format(model.best_score, model.best_iteration+1))\n",
    "        \n",
    "        # Evaluating the model\n",
    "        predictions_, probabilities_ = model.predict(dtest).argmax(axis=1), model.predict(dtest)\n",
    "        \n",
    "        # Calculating metrics\n",
    "        f1_score, precision, recall, auc, _ = metrics.metrics_function(y_predicted=predictions_, \n",
    "                                                                        y_probs=probabilities_, \n",
    "                                                                        y_true=test_set['Label'])\n",
    "        \n",
    "        # Appending metrics\n",
    "        f1_scores.append(f1_score)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        aucs.append(auc)\n",
    "    \n",
    "    # Compiling results from all trials and exporting to a CSV\n",
    "    print(f\"Compiling Results from {iterations} trials...\")\n",
    "    compiled_numeric = metrics.compile_numeric_results(f1_scores, precisions, recalls, aucs)\n",
    "    compiled_numeric.index.name = 'Score'\n",
    "    compiled_numeric.to_csv(f\"results/baselines/metrics/xgboost-view{view}-{suffix}\"+ \"_numeric.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'min_child_weight': 5, 'objective': 'multi:softprob', 'eval_metric': 'mlogloss', 'num_class': 4, 'max_depth': 6, 'subsample': 1, 'gamma': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0520cb0b544ff6ba47fa64a786ca4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Up Data...\n",
      "Train Shape (937, 65537)\n",
      "Val Shape (261, 65537)\n",
      "Test Shape (445, 65537)\n",
      "Training Final XGB Model...\n",
      "[0]\tVal-mlogloss:1.30924\n",
      "Will train until Val-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\tVal-mlogloss:1.27750\n",
      "[2]\tVal-mlogloss:1.23857\n",
      "[3]\tVal-mlogloss:1.20193\n",
      "[4]\tVal-mlogloss:1.19100\n",
      "[5]\tVal-mlogloss:1.15992\n",
      "[6]\tVal-mlogloss:1.13921\n",
      "[7]\tVal-mlogloss:1.11893\n",
      "[8]\tVal-mlogloss:1.09957\n",
      "[9]\tVal-mlogloss:1.09482\n",
      "[10]\tVal-mlogloss:1.08626\n",
      "[11]\tVal-mlogloss:1.08291\n",
      "[12]\tVal-mlogloss:1.07329\n",
      "[13]\tVal-mlogloss:1.06486\n",
      "[14]\tVal-mlogloss:1.06372\n",
      "[15]\tVal-mlogloss:1.06472\n",
      "[16]\tVal-mlogloss:1.06096\n",
      "[17]\tVal-mlogloss:1.05462\n",
      "[18]\tVal-mlogloss:1.05148\n",
      "[19]\tVal-mlogloss:1.05083\n",
      "[20]\tVal-mlogloss:1.04801\n",
      "[21]\tVal-mlogloss:1.04942\n",
      "[22]\tVal-mlogloss:1.05055\n",
      "[23]\tVal-mlogloss:1.05013\n",
      "[24]\tVal-mlogloss:1.05301\n",
      "[25]\tVal-mlogloss:1.05277\n",
      "[26]\tVal-mlogloss:1.05260\n",
      "[27]\tVal-mlogloss:1.05248\n",
      "[28]\tVal-mlogloss:1.05240\n",
      "[29]\tVal-mlogloss:1.05234\n",
      "[30]\tVal-mlogloss:1.05231\n",
      "Stopping. Best iteration:\n",
      "[20]\tVal-mlogloss:1.04801\n",
      "\n",
      "Best MLogloss: 1.05 with 21 rounds\n",
      "Setting Up Data...\n",
      "Train Shape (939, 65537)\n",
      "Val Shape (210, 65537)\n",
      "Test Shape (494, 65537)\n",
      "Training Final XGB Model...\n",
      "[0]\tVal-mlogloss:1.31489\n",
      "Will train until Val-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\tVal-mlogloss:1.26469\n",
      "[2]\tVal-mlogloss:1.22952\n",
      "[3]\tVal-mlogloss:1.20815\n",
      "[4]\tVal-mlogloss:1.19226\n",
      "[5]\tVal-mlogloss:1.16350\n",
      "[6]\tVal-mlogloss:1.15037\n",
      "[7]\tVal-mlogloss:1.13067\n",
      "[8]\tVal-mlogloss:1.14080\n",
      "[9]\tVal-mlogloss:1.13027\n",
      "[10]\tVal-mlogloss:1.13237\n",
      "[11]\tVal-mlogloss:1.13242\n",
      "[12]\tVal-mlogloss:1.13877\n",
      "[13]\tVal-mlogloss:1.13517\n",
      "[14]\tVal-mlogloss:1.13751\n",
      "[15]\tVal-mlogloss:1.13467\n",
      "[16]\tVal-mlogloss:1.13221\n",
      "[17]\tVal-mlogloss:1.13046\n",
      "[18]\tVal-mlogloss:1.13309\n",
      "[19]\tVal-mlogloss:1.13237\n",
      "Stopping. Best iteration:\n",
      "[9]\tVal-mlogloss:1.13027\n",
      "\n",
      "Best MLogloss: 1.13 with 10 rounds\n",
      "Setting Up Data...\n",
      "Train Shape (1038, 65537)\n",
      "Val Shape (237, 65537)\n",
      "Test Shape (368, 65537)\n",
      "Training Final XGB Model...\n",
      "[0]\tVal-mlogloss:1.25977\n",
      "Will train until Val-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\tVal-mlogloss:1.17166\n",
      "[2]\tVal-mlogloss:1.10581\n",
      "[3]\tVal-mlogloss:1.05806\n",
      "[4]\tVal-mlogloss:1.03592\n",
      "[5]\tVal-mlogloss:1.02251\n",
      "[6]\tVal-mlogloss:1.00233\n",
      "[7]\tVal-mlogloss:0.98884\n",
      "[8]\tVal-mlogloss:0.97945\n",
      "[9]\tVal-mlogloss:0.97424\n",
      "[10]\tVal-mlogloss:0.97239\n",
      "[11]\tVal-mlogloss:0.96691\n",
      "[12]\tVal-mlogloss:0.96931\n",
      "[13]\tVal-mlogloss:0.97657\n",
      "[14]\tVal-mlogloss:0.97942\n",
      "[15]\tVal-mlogloss:0.98374\n",
      "[16]\tVal-mlogloss:0.98730\n",
      "[17]\tVal-mlogloss:0.99482\n",
      "[18]\tVal-mlogloss:0.99832\n",
      "[19]\tVal-mlogloss:1.00151\n",
      "[20]\tVal-mlogloss:1.00479\n",
      "[21]\tVal-mlogloss:1.00832\n",
      "Stopping. Best iteration:\n",
      "[11]\tVal-mlogloss:0.96691\n",
      "\n",
      "Best MLogloss: 0.97 with 12 rounds\n",
      "Setting Up Data...\n",
      "Train Shape (976, 65537)\n",
      "Val Shape (222, 65537)\n",
      "Test Shape (445, 65537)\n",
      "Training Final XGB Model...\n",
      "[0]\tVal-mlogloss:1.30167\n",
      "Will train until Val-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\tVal-mlogloss:1.22748\n",
      "[2]\tVal-mlogloss:1.16928\n",
      "[3]\tVal-mlogloss:1.14096\n",
      "[4]\tVal-mlogloss:1.12096\n",
      "[5]\tVal-mlogloss:1.09183\n",
      "[6]\tVal-mlogloss:1.07479\n",
      "[7]\tVal-mlogloss:1.05880\n",
      "[8]\tVal-mlogloss:1.04673\n",
      "[9]\tVal-mlogloss:1.04216\n",
      "[10]\tVal-mlogloss:1.04273\n",
      "[11]\tVal-mlogloss:1.04017\n",
      "[12]\tVal-mlogloss:1.04308\n",
      "[13]\tVal-mlogloss:1.04493\n",
      "[14]\tVal-mlogloss:1.04764\n",
      "[15]\tVal-mlogloss:1.04865\n",
      "[16]\tVal-mlogloss:1.05012\n",
      "[17]\tVal-mlogloss:1.04911\n",
      "[18]\tVal-mlogloss:1.04709\n",
      "[19]\tVal-mlogloss:1.04732\n",
      "[20]\tVal-mlogloss:1.04757\n",
      "[21]\tVal-mlogloss:1.04797\n",
      "Stopping. Best iteration:\n",
      "[11]\tVal-mlogloss:1.04017\n",
      "\n",
      "Best MLogloss: 1.04 with 12 rounds\n",
      "Setting Up Data...\n",
      "Train Shape (1009, 65537)\n",
      "Val Shape (199, 65537)\n",
      "Test Shape (435, 65537)\n",
      "Training Final XGB Model...\n",
      "[0]\tVal-mlogloss:1.27198\n",
      "Will train until Val-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\tVal-mlogloss:1.21701\n",
      "[2]\tVal-mlogloss:1.17295\n",
      "[3]\tVal-mlogloss:1.13545\n",
      "[4]\tVal-mlogloss:1.10440\n",
      "[5]\tVal-mlogloss:1.08619\n",
      "[6]\tVal-mlogloss:1.07631\n",
      "[7]\tVal-mlogloss:1.05073\n",
      "[8]\tVal-mlogloss:1.04142\n",
      "[9]\tVal-mlogloss:1.04216\n",
      "[10]\tVal-mlogloss:1.03089\n",
      "[11]\tVal-mlogloss:1.02647\n",
      "[12]\tVal-mlogloss:1.02170\n",
      "[13]\tVal-mlogloss:1.01412\n",
      "[14]\tVal-mlogloss:1.00915\n",
      "[15]\tVal-mlogloss:1.00290\n",
      "[16]\tVal-mlogloss:1.00237\n",
      "[17]\tVal-mlogloss:1.00176\n",
      "[18]\tVal-mlogloss:0.99997\n",
      "[19]\tVal-mlogloss:0.99949\n",
      "[20]\tVal-mlogloss:0.99694\n",
      "[21]\tVal-mlogloss:0.99545\n",
      "[22]\tVal-mlogloss:0.99455\n",
      "[23]\tVal-mlogloss:0.99383\n",
      "[24]\tVal-mlogloss:0.99326\n",
      "[25]\tVal-mlogloss:0.99280\n",
      "[26]\tVal-mlogloss:0.99244\n",
      "[27]\tVal-mlogloss:0.99215\n",
      "[28]\tVal-mlogloss:0.99191\n",
      "[29]\tVal-mlogloss:0.99172\n",
      "[30]\tVal-mlogloss:0.99157\n",
      "[31]\tVal-mlogloss:0.99144\n",
      "[32]\tVal-mlogloss:0.99134\n",
      "[33]\tVal-mlogloss:0.99126\n",
      "[34]\tVal-mlogloss:0.99119\n",
      "[35]\tVal-mlogloss:0.99113\n",
      "[36]\tVal-mlogloss:0.99109\n",
      "[37]\tVal-mlogloss:0.99105\n",
      "[38]\tVal-mlogloss:0.99102\n",
      "[39]\tVal-mlogloss:0.99099\n",
      "[40]\tVal-mlogloss:0.99097\n",
      "[41]\tVal-mlogloss:0.99095\n",
      "[42]\tVal-mlogloss:0.99094\n",
      "[43]\tVal-mlogloss:0.99093\n",
      "[44]\tVal-mlogloss:0.99092\n",
      "[45]\tVal-mlogloss:0.99091\n",
      "[46]\tVal-mlogloss:0.99090\n",
      "[47]\tVal-mlogloss:0.99089\n",
      "[48]\tVal-mlogloss:0.99089\n",
      "[49]\tVal-mlogloss:0.99089\n",
      "[50]\tVal-mlogloss:0.99088\n",
      "[51]\tVal-mlogloss:0.99088\n",
      "[52]\tVal-mlogloss:0.99088\n",
      "[53]\tVal-mlogloss:0.99087\n",
      "[54]\tVal-mlogloss:0.99087\n",
      "[55]\tVal-mlogloss:0.99087\n",
      "[56]\tVal-mlogloss:0.99087\n",
      "[57]\tVal-mlogloss:0.99087\n",
      "[58]\tVal-mlogloss:0.99087\n",
      "[59]\tVal-mlogloss:0.99087\n",
      "[60]\tVal-mlogloss:0.99087\n",
      "[61]\tVal-mlogloss:0.99087\n",
      "[62]\tVal-mlogloss:0.99087\n",
      "[63]\tVal-mlogloss:0.99087\n",
      "[64]\tVal-mlogloss:0.99087\n",
      "[65]\tVal-mlogloss:0.99087\n",
      "[66]\tVal-mlogloss:0.99086\n",
      "[67]\tVal-mlogloss:0.99086\n",
      "[68]\tVal-mlogloss:0.99086\n",
      "[69]\tVal-mlogloss:0.99086\n",
      "[70]\tVal-mlogloss:0.99086\n",
      "[71]\tVal-mlogloss:0.99086\n",
      "[72]\tVal-mlogloss:0.99086\n",
      "[73]\tVal-mlogloss:0.99086\n",
      "[74]\tVal-mlogloss:0.99086\n",
      "[75]\tVal-mlogloss:0.99086\n",
      "[76]\tVal-mlogloss:0.99086\n",
      "Stopping. Best iteration:\n",
      "[66]\tVal-mlogloss:0.99086\n",
      "\n",
      "Best MLogloss: 0.99 with 67 rounds\n",
      "Setting Up Data...\n",
      "Train Shape (988, 65537)\n",
      "Val Shape (228, 65537)\n",
      "Test Shape (427, 65537)\n",
      "Training Final XGB Model...\n",
      "[0]\tVal-mlogloss:1.26249\n",
      "Will train until Val-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\tVal-mlogloss:1.19251\n",
      "[2]\tVal-mlogloss:1.13013\n",
      "[3]\tVal-mlogloss:1.09586\n",
      "[4]\tVal-mlogloss:1.06297\n",
      "[5]\tVal-mlogloss:1.04613\n",
      "[6]\tVal-mlogloss:1.01991\n",
      "[7]\tVal-mlogloss:1.00965\n",
      "[8]\tVal-mlogloss:0.99086\n",
      "[9]\tVal-mlogloss:0.98299\n",
      "[10]\tVal-mlogloss:0.97940\n",
      "[11]\tVal-mlogloss:0.97488\n",
      "[12]\tVal-mlogloss:0.96911\n",
      "[13]\tVal-mlogloss:0.96803\n",
      "[14]\tVal-mlogloss:0.96557\n",
      "[15]\tVal-mlogloss:0.96232\n",
      "[16]\tVal-mlogloss:0.95955\n",
      "[17]\tVal-mlogloss:0.95575\n",
      "[18]\tVal-mlogloss:0.95600\n",
      "[19]\tVal-mlogloss:0.95268\n",
      "[20]\tVal-mlogloss:0.95144\n",
      "[21]\tVal-mlogloss:0.95044\n",
      "[22]\tVal-mlogloss:0.94964\n",
      "[23]\tVal-mlogloss:0.94899\n",
      "[24]\tVal-mlogloss:0.94847\n",
      "[25]\tVal-mlogloss:0.94804\n",
      "[26]\tVal-mlogloss:0.94769\n",
      "[27]\tVal-mlogloss:0.94741\n",
      "[28]\tVal-mlogloss:0.94718\n",
      "[29]\tVal-mlogloss:0.94699\n",
      "[30]\tVal-mlogloss:0.94684\n",
      "[31]\tVal-mlogloss:0.94671\n",
      "[32]\tVal-mlogloss:0.94660\n",
      "[33]\tVal-mlogloss:0.94651\n",
      "[34]\tVal-mlogloss:0.94644\n",
      "[35]\tVal-mlogloss:0.94638\n",
      "[36]\tVal-mlogloss:0.94633\n",
      "[37]\tVal-mlogloss:0.94629\n",
      "[38]\tVal-mlogloss:0.94626\n",
      "[39]\tVal-mlogloss:0.94623\n",
      "[40]\tVal-mlogloss:0.94620\n",
      "[41]\tVal-mlogloss:0.94618\n",
      "[42]\tVal-mlogloss:0.94617\n",
      "[43]\tVal-mlogloss:0.94615\n",
      "[44]\tVal-mlogloss:0.94614\n",
      "[45]\tVal-mlogloss:0.94613\n",
      "[46]\tVal-mlogloss:0.94612\n",
      "[47]\tVal-mlogloss:0.94612\n",
      "[48]\tVal-mlogloss:0.94611\n",
      "[49]\tVal-mlogloss:0.94611\n",
      "[50]\tVal-mlogloss:0.94610\n",
      "[51]\tVal-mlogloss:0.94610\n",
      "[52]\tVal-mlogloss:0.94610\n",
      "[53]\tVal-mlogloss:0.94610\n",
      "[54]\tVal-mlogloss:0.94609\n",
      "[55]\tVal-mlogloss:0.94609\n",
      "[56]\tVal-mlogloss:0.94609\n",
      "[57]\tVal-mlogloss:0.94609\n",
      "[58]\tVal-mlogloss:0.94609\n",
      "[59]\tVal-mlogloss:0.94609\n",
      "[60]\tVal-mlogloss:0.94609\n",
      "[61]\tVal-mlogloss:0.94608\n",
      "[62]\tVal-mlogloss:0.94608\n",
      "[63]\tVal-mlogloss:0.94608\n",
      "[64]\tVal-mlogloss:0.94608\n",
      "[65]\tVal-mlogloss:0.94608\n",
      "[66]\tVal-mlogloss:0.94608\n",
      "[67]\tVal-mlogloss:0.94608\n",
      "[68]\tVal-mlogloss:0.94608\n",
      "[69]\tVal-mlogloss:0.94608\n",
      "[70]\tVal-mlogloss:0.94608\n",
      "[71]\tVal-mlogloss:0.94608\n",
      "[72]\tVal-mlogloss:0.94608\n",
      "[73]\tVal-mlogloss:0.94608\n",
      "[74]\tVal-mlogloss:0.94608\n",
      "[75]\tVal-mlogloss:0.94608\n",
      "[76]\tVal-mlogloss:0.94608\n",
      "[77]\tVal-mlogloss:0.94608\n",
      "[78]\tVal-mlogloss:0.94608\n",
      "[79]\tVal-mlogloss:0.94608\n",
      "[80]\tVal-mlogloss:0.94608\n",
      "[81]\tVal-mlogloss:0.94608\n",
      "[82]\tVal-mlogloss:0.94608\n",
      "[83]\tVal-mlogloss:0.94608\n",
      "[84]\tVal-mlogloss:0.94608\n",
      "Stopping. Best iteration:\n",
      "[74]\tVal-mlogloss:0.94608\n",
      "\n",
      "Best MLogloss: 0.95 with 75 rounds\n",
      "Setting Up Data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape (992, 65537)\n",
      "Val Shape (224, 65537)\n",
      "Test Shape (427, 65537)\n",
      "Training Final XGB Model...\n",
      "[0]\tVal-mlogloss:1.26286\n",
      "Will train until Val-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\tVal-mlogloss:1.16115\n",
      "[2]\tVal-mlogloss:1.09050\n",
      "[3]\tVal-mlogloss:1.04857\n",
      "[4]\tVal-mlogloss:1.01770\n",
      "[5]\tVal-mlogloss:0.98913\n",
      "[6]\tVal-mlogloss:0.97107\n",
      "[7]\tVal-mlogloss:0.95565\n",
      "[8]\tVal-mlogloss:0.94514\n",
      "[9]\tVal-mlogloss:0.93301\n",
      "[10]\tVal-mlogloss:0.92695\n",
      "[11]\tVal-mlogloss:0.92088\n",
      "[12]\tVal-mlogloss:0.91991\n",
      "[13]\tVal-mlogloss:0.91757\n",
      "[14]\tVal-mlogloss:0.91993\n",
      "[15]\tVal-mlogloss:0.91743\n",
      "[16]\tVal-mlogloss:0.91656\n",
      "[17]\tVal-mlogloss:0.91813\n",
      "[18]\tVal-mlogloss:0.91769\n",
      "[19]\tVal-mlogloss:0.91799\n",
      "[20]\tVal-mlogloss:0.91768\n",
      "[21]\tVal-mlogloss:0.91681\n",
      "[22]\tVal-mlogloss:0.91658\n",
      "[23]\tVal-mlogloss:0.91639\n",
      "[24]\tVal-mlogloss:0.91623\n",
      "[25]\tVal-mlogloss:0.91609\n",
      "[26]\tVal-mlogloss:0.91597\n",
      "[27]\tVal-mlogloss:0.91542\n",
      "[28]\tVal-mlogloss:0.91358\n",
      "[29]\tVal-mlogloss:0.91350\n",
      "[30]\tVal-mlogloss:0.91343\n",
      "[31]\tVal-mlogloss:0.91338\n",
      "[32]\tVal-mlogloss:0.91333\n",
      "[33]\tVal-mlogloss:0.91329\n",
      "[34]\tVal-mlogloss:0.91325\n",
      "[35]\tVal-mlogloss:0.91322\n",
      "[36]\tVal-mlogloss:0.91319\n",
      "[37]\tVal-mlogloss:0.91317\n",
      "[38]\tVal-mlogloss:0.91315\n",
      "[39]\tVal-mlogloss:0.91314\n",
      "[40]\tVal-mlogloss:0.91312\n",
      "[41]\tVal-mlogloss:0.91311\n",
      "[42]\tVal-mlogloss:0.91311\n",
      "[43]\tVal-mlogloss:0.91310\n",
      "[44]\tVal-mlogloss:0.91309\n",
      "[45]\tVal-mlogloss:0.91309\n",
      "[46]\tVal-mlogloss:0.91308\n",
      "[47]\tVal-mlogloss:0.91308\n",
      "[48]\tVal-mlogloss:0.91307\n",
      "[49]\tVal-mlogloss:0.91307\n",
      "[50]\tVal-mlogloss:0.91307\n",
      "[51]\tVal-mlogloss:0.91307\n",
      "[52]\tVal-mlogloss:0.91306\n",
      "[53]\tVal-mlogloss:0.91306\n",
      "[54]\tVal-mlogloss:0.91306\n",
      "[55]\tVal-mlogloss:0.91306\n",
      "[56]\tVal-mlogloss:0.91306\n",
      "[57]\tVal-mlogloss:0.91306\n",
      "[58]\tVal-mlogloss:0.91306\n",
      "[59]\tVal-mlogloss:0.91306\n",
      "[60]\tVal-mlogloss:0.91306\n",
      "[61]\tVal-mlogloss:0.91306\n",
      "[62]\tVal-mlogloss:0.91306\n",
      "[63]\tVal-mlogloss:0.91306\n",
      "[64]\tVal-mlogloss:0.91306\n",
      "[65]\tVal-mlogloss:0.91306\n",
      "[66]\tVal-mlogloss:0.91306\n",
      "[67]\tVal-mlogloss:0.91306\n",
      "[68]\tVal-mlogloss:0.91306\n",
      "[69]\tVal-mlogloss:0.91306\n",
      "[70]\tVal-mlogloss:0.91306\n",
      "[71]\tVal-mlogloss:0.91306\n",
      "[72]\tVal-mlogloss:0.91306\n",
      "[73]\tVal-mlogloss:0.91306\n",
      "[74]\tVal-mlogloss:0.91306\n",
      "[75]\tVal-mlogloss:0.91306\n",
      "[76]\tVal-mlogloss:0.91306\n",
      "[77]\tVal-mlogloss:0.91306\n",
      "[78]\tVal-mlogloss:0.91306\n",
      "Stopping. Best iteration:\n",
      "[68]\tVal-mlogloss:0.91306\n",
      "\n",
      "Best MLogloss: 0.91 with 69 rounds\n",
      "Setting Up Data...\n",
      "Train Shape (893, 65537)\n",
      "Val Shape (253, 65537)\n",
      "Test Shape (497, 65537)\n",
      "Training Final XGB Model...\n",
      "[0]\tVal-mlogloss:1.30121\n",
      "Will train until Val-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\tVal-mlogloss:1.22874\n",
      "[2]\tVal-mlogloss:1.18988\n",
      "[3]\tVal-mlogloss:1.15382\n",
      "[4]\tVal-mlogloss:1.14669\n",
      "[5]\tVal-mlogloss:1.11317\n",
      "[6]\tVal-mlogloss:1.09859\n",
      "[7]\tVal-mlogloss:1.09268\n",
      "[8]\tVal-mlogloss:1.07451\n",
      "[9]\tVal-mlogloss:1.06051\n",
      "[10]\tVal-mlogloss:1.05485\n",
      "[11]\tVal-mlogloss:1.05208\n",
      "[12]\tVal-mlogloss:1.04694\n",
      "[13]\tVal-mlogloss:1.04594\n",
      "[14]\tVal-mlogloss:1.04139\n",
      "[15]\tVal-mlogloss:1.03825\n",
      "[16]\tVal-mlogloss:1.03636\n",
      "[17]\tVal-mlogloss:1.03327\n",
      "[18]\tVal-mlogloss:1.03192\n",
      "[19]\tVal-mlogloss:1.03085\n",
      "[20]\tVal-mlogloss:1.03180\n",
      "[21]\tVal-mlogloss:1.03103\n",
      "[22]\tVal-mlogloss:1.03042\n",
      "[23]\tVal-mlogloss:1.03068\n",
      "[24]\tVal-mlogloss:1.03025\n",
      "[25]\tVal-mlogloss:1.02991\n",
      "[26]\tVal-mlogloss:1.02963\n",
      "[27]\tVal-mlogloss:1.02940\n",
      "[28]\tVal-mlogloss:1.02922\n",
      "[29]\tVal-mlogloss:1.02907\n",
      "[30]\tVal-mlogloss:1.02894\n",
      "[31]\tVal-mlogloss:1.02884\n",
      "[32]\tVal-mlogloss:1.02876\n",
      "[33]\tVal-mlogloss:1.02869\n",
      "[34]\tVal-mlogloss:1.02863\n",
      "[35]\tVal-mlogloss:1.02859\n",
      "[36]\tVal-mlogloss:1.02855\n",
      "[37]\tVal-mlogloss:1.02852\n",
      "[38]\tVal-mlogloss:1.02849\n",
      "[39]\tVal-mlogloss:1.02847\n",
      "[40]\tVal-mlogloss:1.02845\n",
      "[41]\tVal-mlogloss:1.02843\n",
      "[42]\tVal-mlogloss:1.02842\n",
      "[43]\tVal-mlogloss:1.02841\n",
      "[44]\tVal-mlogloss:1.02840\n",
      "[45]\tVal-mlogloss:1.02839\n",
      "[46]\tVal-mlogloss:1.02839\n",
      "[47]\tVal-mlogloss:1.02838\n",
      "[48]\tVal-mlogloss:1.02838\n",
      "[49]\tVal-mlogloss:1.02837\n",
      "[50]\tVal-mlogloss:1.02837\n",
      "[51]\tVal-mlogloss:1.02837\n",
      "[52]\tVal-mlogloss:1.02836\n",
      "[53]\tVal-mlogloss:1.02836\n",
      "[54]\tVal-mlogloss:1.02836\n",
      "[55]\tVal-mlogloss:1.02836\n",
      "[56]\tVal-mlogloss:1.02836\n",
      "[57]\tVal-mlogloss:1.02836\n",
      "[58]\tVal-mlogloss:1.02836\n",
      "[59]\tVal-mlogloss:1.02836\n",
      "[60]\tVal-mlogloss:1.02836\n",
      "[61]\tVal-mlogloss:1.02836\n",
      "[62]\tVal-mlogloss:1.02836\n",
      "[63]\tVal-mlogloss:1.02836\n",
      "[64]\tVal-mlogloss:1.02835\n",
      "[65]\tVal-mlogloss:1.02835\n",
      "[66]\tVal-mlogloss:1.02835\n",
      "[67]\tVal-mlogloss:1.02835\n",
      "[68]\tVal-mlogloss:1.02835\n",
      "[69]\tVal-mlogloss:1.02835\n",
      "[70]\tVal-mlogloss:1.02835\n",
      "[71]\tVal-mlogloss:1.02835\n",
      "[72]\tVal-mlogloss:1.02835\n",
      "[73]\tVal-mlogloss:1.02835\n",
      "[74]\tVal-mlogloss:1.02835\n",
      "[75]\tVal-mlogloss:1.02835\n",
      "[76]\tVal-mlogloss:1.02835\n",
      "[77]\tVal-mlogloss:1.02835\n",
      "[78]\tVal-mlogloss:1.02835\n",
      "[79]\tVal-mlogloss:1.02835\n",
      "[80]\tVal-mlogloss:1.02835\n",
      "[81]\tVal-mlogloss:1.02835\n",
      "Stopping. Best iteration:\n",
      "[71]\tVal-mlogloss:1.02835\n",
      "\n",
      "Best MLogloss: 1.03 with 72 rounds\n",
      "Setting Up Data...\n",
      "Train Shape (926, 65537)\n",
      "Val Shape (331, 65537)\n",
      "Test Shape (386, 65537)\n",
      "Training Final XGB Model...\n",
      "[0]\tVal-mlogloss:1.27029\n",
      "Will train until Val-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\tVal-mlogloss:1.19760\n",
      "[2]\tVal-mlogloss:1.13794\n",
      "[3]\tVal-mlogloss:1.10543\n",
      "[4]\tVal-mlogloss:1.06731\n",
      "[5]\tVal-mlogloss:1.04343\n",
      "[6]\tVal-mlogloss:1.01077\n",
      "[7]\tVal-mlogloss:0.99572\n",
      "[8]\tVal-mlogloss:0.98582\n",
      "[9]\tVal-mlogloss:0.97237\n",
      "[10]\tVal-mlogloss:0.96387\n",
      "[11]\tVal-mlogloss:0.95743\n",
      "[12]\tVal-mlogloss:0.95791\n",
      "[13]\tVal-mlogloss:0.95717\n",
      "[14]\tVal-mlogloss:0.95440\n",
      "[15]\tVal-mlogloss:0.95972\n",
      "[16]\tVal-mlogloss:0.95829\n",
      "[17]\tVal-mlogloss:0.95776\n",
      "[18]\tVal-mlogloss:0.95957\n",
      "[19]\tVal-mlogloss:0.95891\n",
      "[20]\tVal-mlogloss:0.96265\n",
      "[21]\tVal-mlogloss:0.96343\n",
      "[22]\tVal-mlogloss:0.96318\n",
      "[23]\tVal-mlogloss:0.96305\n",
      "[24]\tVal-mlogloss:0.96565\n",
      "Stopping. Best iteration:\n",
      "[14]\tVal-mlogloss:0.95440\n",
      "\n",
      "Best MLogloss: 0.95 with 15 rounds\n",
      "Setting Up Data...\n",
      "Train Shape (945, 65537)\n",
      "Val Shape (236, 65537)\n",
      "Test Shape (462, 65537)\n",
      "Training Final XGB Model...\n",
      "[0]\tVal-mlogloss:1.31310\n",
      "Will train until Val-mlogloss hasn't improved in 10 rounds.\n",
      "[1]\tVal-mlogloss:1.24116\n",
      "[2]\tVal-mlogloss:1.20005\n",
      "[3]\tVal-mlogloss:1.16322\n",
      "[4]\tVal-mlogloss:1.14688\n",
      "[5]\tVal-mlogloss:1.12254\n",
      "[6]\tVal-mlogloss:1.10479\n",
      "[7]\tVal-mlogloss:1.08426\n",
      "[8]\tVal-mlogloss:1.07692\n",
      "[9]\tVal-mlogloss:1.05585\n",
      "[10]\tVal-mlogloss:1.04619\n",
      "[11]\tVal-mlogloss:1.03406\n",
      "[12]\tVal-mlogloss:1.02474\n",
      "[13]\tVal-mlogloss:1.02705\n",
      "[14]\tVal-mlogloss:1.02585\n",
      "[15]\tVal-mlogloss:1.02273\n",
      "[16]\tVal-mlogloss:1.02142\n",
      "[17]\tVal-mlogloss:1.02188\n",
      "[18]\tVal-mlogloss:1.02535\n",
      "[19]\tVal-mlogloss:1.02580\n",
      "[20]\tVal-mlogloss:1.02738\n",
      "[21]\tVal-mlogloss:1.02925\n",
      "[22]\tVal-mlogloss:1.02821\n",
      "[23]\tVal-mlogloss:1.02818\n",
      "[24]\tVal-mlogloss:1.03070\n",
      "[25]\tVal-mlogloss:1.03076\n",
      "[26]\tVal-mlogloss:1.03472\n",
      "Stopping. Best iteration:\n",
      "[16]\tVal-mlogloss:1.02142\n",
      "\n",
      "Best MLogloss: 1.02 with 17 rounds\n",
      "\n",
      "Compiling Results from 10 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-utoronto_spine/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "trainWithSignificance(data, suffix, view, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = 'posterior'\n",
    "view=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Confidence Intervals</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.408972</td>\n",
       "      <td>[0.3558041901048561, 0.4621405620657587]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.430244</td>\n",
       "      <td>[0.3441836171383514, 0.5163037535675206]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.427084</td>\n",
       "      <td>[0.37072581624311063, 0.48344170108697304]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUC</td>\n",
       "      <td>0.482312</td>\n",
       "      <td>[0.43351183950255046, 0.5311119427621224]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score      Mean                        Confidence Intervals  Support\n",
       "0         F1  0.408972    [0.3558041901048561, 0.4621405620657587]       10\n",
       "1  Precision  0.430244    [0.3441836171383514, 0.5163037535675206]       10\n",
       "2     Recall  0.427084  [0.37072581624311063, 0.48344170108697304]       10\n",
       "3        AUC  0.482312   [0.43351183950255046, 0.5311119427621224]       10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.read_csv(f\"results/baselines/metrics/xgboost-view{view}-{suffix}\"+ \"_numeric.csv\")\n",
    "results['Confidence Intervals'] =results['Confidence Intervals'].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Mean</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.408972</td>\n",
       "      <td>[-0.05316818598045131, 0.05316818598045131]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.430244</td>\n",
       "      <td>[-0.08606006821458462, 0.08606006821458456]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.427084</td>\n",
       "      <td>[-0.056357942421931095, 0.05635794242193132]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUC</td>\n",
       "      <td>0.482312</td>\n",
       "      <td>[-0.04880005162978596, 0.04880005162978596]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score      Mean                                             0\n",
       "0         F1  0.408972   [-0.05316818598045131, 0.05316818598045131]\n",
       "1  Precision  0.430244   [-0.08606006821458462, 0.08606006821458456]\n",
       "2     Recall  0.427084  [-0.056357942421931095, 0.05635794242193132]\n",
       "3        AUC  0.482312   [-0.04880005162978596, 0.04880005162978596]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([results[['Score','Mean']], results['Confidence Intervals'] - results['Mean']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = 'posterior'\n",
    "view=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'min_child_weight': 5, 'objective': 'multi:softprob', 'eval_metric': 'mlogloss', 'max_depth': 8, 'subsample': 1, 'gamma': 2}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f\"results/baselines/tuning/xgboosttuning-{suffix}-view{view}.csv\")\n",
    "best_params = df[['max_depth', 'subsample', 'gamma']].to_dict('records')[0] # Hard coding!\n",
    "params = {'min_child_weight': 5, \n",
    "                'objective': 'multi:softprob', \n",
    "                'eval_metric': 'mlogloss'}\n",
    "params.update(best_params)\n",
    "print(\"Parameters:\", params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
